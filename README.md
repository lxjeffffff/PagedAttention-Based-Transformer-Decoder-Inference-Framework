PagedAttention-Based Transformer Decoder Inference Framework

This project is a modular, production-ready framework for running high-performance transformer decoder inference powered by **PagedAttention**.

It supports both GPU (FlashAttention-style fused kernels) and CPU (INT8 vectorized tile kernels with oneDNN). The system integrates token streaming, beam search, reranking, KV tile caching, and REST/CLI/Web-based access.

---

## ğŸš€ Core Highlights

- âœ… **PagedAttention** with tile-based KV caching
- âœ… GPU FlashAttention-style fused kernels (`paged_flash_attention_kernel_fused`)
- âœ… CPU INT8 inference with AVX2 + LUT softmax + oneDNN
- âœ… Supports single/multi-turn generation, streaming, rerank
- âœ… Unified `CUDADecoder` and `INT8Decoder` APIs
- âœ… HuggingFace tokenizer + caching
- âœ… Beam search reranker + JSONL export + finetuning

---

## ğŸ“ Project Structure Overview

```
project_root/
â”œâ”€â”€ api/           # FastAPI-based inference server
â”œâ”€â”€ web/           # Flask SSE web interface
â”œâ”€â”€ cli/           # CLI tools for generate, chat, batch, rerank
â”œâ”€â”€ decoder/       # Core decoder blocks (CUDADecoder, INT8Decoder)
â”œâ”€â”€ attention/     # GPU FlashAttention kernels (PagedAttention)
â”œâ”€â”€ attention_cpu/ # CPU kernels (INT8 + vectorized softmax)
â”œâ”€â”€ kv_cache/      # KV tile cache with page table (GPU + CPU)
â”œâ”€â”€ reranker/      # Beam reranker + finetuning tools
â”œâ”€â”€ config/        # Model architecture + runtime configuration
â”œâ”€â”€ weights/       # Placeholder for model weights
â”œâ”€â”€ include/       # pybind11 headers
â”œâ”€â”€ src/           # pybind11 binding logic
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ CMakeLists.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md      # You are here
```

---

## ğŸ§  What is PagedAttention?

PagedAttention refers to an attention computation mechanism where the KV cache is broken into **tiles (pages)**, and only necessary tiles are fetched during streaming inference. This:

- Reduces memory reads
- Enables longer context windows
- Supports efficient beam search and reranker interaction
- Powers **streaming + high throughput decoding**

This project implements PagedAttention for both GPU and CPU backends, including:

- Warp-level softmax and AV cache reuse (GPU)
- Tile-wise prefetch and masking (CPU INT8)
- Beam-aware prefetching
- Tile eviction + LRU cache on CPU

---

## ğŸ“¦ Installation

### âœ… Python dependencies

```bash
pip install -r requirements.txt
```

Includes:
- `transformers`, `fastapi`, `pybind11`, `uvicorn`, `flask`, `datasets`, `matplotlib` etc.

---

### âš™ï¸ C++/CUDA build (CMake or setup.py)

#### Option 1: CMake build

```bash
mkdir build && cd build
cmake ..
make -j
```

#### Option 2: Python install

```bash
pip install .
```

Builds and installs `llm_decoder` Python module via pybind11 + C++/CUDA

---

## ğŸ§ª CLI Usage

```bash
python cli/generate_cli.py --prompt "Hello"
python cli/chat_cli.py
python cli/generate_batch.py
python cli/rerank_eval.py
```

Unified launcher:

```bash
python cli/cli_app.py stream
```

---

## ğŸŒ Web / API Inference

### FastAPI server

```bash
uvicorn api.main:app --host 0.0.0.0 --port 8000
```

### Flask SSE server

```bash
python web/app.py
```

Supports:

- `/generate`
- `/stream`
- `/stream_chat_beam`
- `/generate_batch`

---

## ğŸ“„ Configuration Files

Located in `config/`:

| File                     | Purpose |
|--------------------------|---------|
| `model_config.yaml`      | Model layers, heads, dim, tokenizer |
| `runtime_config.yaml`    | Temperature, top_k, backend mode |
| `chat_template.json`     | Assistant prompt + multi-turn format |
| `weight_paths.yaml`      | Per-layer weight locations |
| `paths.json`             | Optional overrides |

---

## ğŸ’¾ Weights Format

Expected under `weights/`:

```
weights/
â”œâ”€â”€ embedding.bin
â”œâ”€â”€ layer_0/
â”‚   â”œâ”€â”€ ln1.bin
â”‚   â”œâ”€â”€ mlp_fc1.bin
â”‚   â””â”€â”€ ...
â”œâ”€â”€ int8/
â”‚   â””â”€â”€ embedding.bin (quantized)
```

Used by `decoder.load_weights(...)` or `int8_decoder.load_quantized_weights(...)`

---

## ğŸ§© Integration Points

- `api/` and `web/` â†’ inference services
- `decoder/` â†’ exposes `generate(...)` for float + int8
- `kv_cache/` â†’ provides paged tile caching with LRU
- `attention/` â†’ GPU FlashAttention (paged)
- `attention_cpu/` â†’ CPU attention kernel (paged)
- `reranker/` â†’ rerank beam candidates, tree visualization, JSONL training

---

## âœ… Status

| Component         | Status |
|-------------------|--------|
| PagedAttention (GPU) | âœ… |
| PagedAttention (CPU) | âœ… |
| CUDA inference    | âœ… Stable |
| INT8 CPU inference | âœ… Stable |
| CLI tools         | âœ… Integrated |
| API / Web         | âœ… Streaming ready |
| Reranker          | âœ… Supported |
| Pybind11 binding  | âœ… Working |
| Beam search       | âœ… with rerank |
| Weights           | âœ… (user-supplied .bin) |

---

## ğŸ“˜ License & Acknowledgements

- FlashAttention concepts inspired by [vLLM](https://github.com/vllm-project/vllm)
- Beam reranking via Transformers + BERT-based scoring
- Quantization powered by oneDNN (INT8 matmul + LUT softmax)
- Tokenizer from HuggingFace (`AutoTokenizer`)
