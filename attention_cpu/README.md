# `attention_cpu/` â€” High-Performance CPU Attention Module (float & int8)

This module implements a CPU-optimized Paged Attention kernel designed for transformer decoders. It supports both `float32` and `int8` inference paths, tile-based KV caching, rotary embeddings, top-k/p sampling, and fast vectorized operations using AVX2/NEON and LUT-based softmax.

---

### ğŸ“¦ Contents

| File                              | Description |
|-----------------------------------|-------------|
| `attention_cpu.hpp/cpp`           | `CPUAttention<T>` wrapper for float/int8 inference |
| `cpu_attention_config.hpp`        | Config struct (head_dim, tile_size, etc.) |
| `cpu_attention_kernel.hpp/cpp`    | Core `cpu_paged_attention_forward<T>()` kernel |
| `attention_kernel_utils_cpu.hpp`  | CPU kernel helpers (masking, rotary, causal, beam EOS) |
| `dnnl_matmul_int8.hpp/cpp`        | INT8 matmul (GEMM) wrapper using oneDNN |
| `int8_quant.hpp/cpp`              | Quantization tools: FP32 â†’ INT8, min/max scaling |
| `softmax_lut.hpp/cpp`             | LUT-accelerated softmax for INT8 inference |
| `vec_cpu.hpp`                     | CPU vector intrinsics abstraction (float/int8 support) |

---

### ğŸš€ Features

- âœ… **Paged Attention** with tile-based KV caching
- âœ… **Rotary Position Embedding (RoPE)**
- âœ… **Causal Masking**
- âœ… **Top-k / Top-p filtering**
- âœ… **EOS token detection**
- âœ… **INT8 inference** with:
  - oneDNN int8 GEMM
  - LUT-based softmax
  - vectorized INT8 arithmetic
- âœ… **Vec<T> CPU vector abstraction**
  - Supports AVX2/AVX512/NEON fallback
- âœ… **Thread-safe and decoder-compatible**

---

### ğŸ§  Architecture

```text
CPUAttention<T>::forward(...)
  â””â”€â”€ cpu_paged_attention_forward<T>(...)
        â”œâ”€â”€ Vec<T> operations
        â”œâ”€â”€ rotary_embed(q, k)
        â”œâ”€â”€ qk_sim = q â€¢ k^T
        â”œâ”€â”€ softmax_lut()  (int8) / softmax_exp() (float)
        â”œâ”€â”€ Top-k / Top-p filter
        â””â”€â”€ AV = Î£ softmax(qk) * V
```

---

### ğŸ§© Integration with Decoder

This module is integrated by the decoder layer:

```cpp
CPUAttention<float> attention(config);
CPUAttentionInput<float> input = { ... };
attention.forward(input, output);
```

Input/output struct supports:

- float/int8 input Q/K/V
- KV tile cache
- optional rotary table
- LUT table
- output buffer for logits, attention weights

---

### âš™ï¸ INT8 Inference Stack

- Quantization via `int8_quant`
- GEMM via `dnnl_matmul_int8`
- Activation via LUT (GELU)
- Softmax via LUT + warp-style parallelism
- Top-k/p via fast filter kernel

---

### ğŸ“„ Example: Launch Kernel

```cpp
cpu_paged_attention_forward<float>(
  input.q, input.k, input.v,
  input.rotary_emb,
  input.kv_cache,
  input.lut,
  B, H, D, T,
  tile_size, top_k, top_p,
  input.temperature, input.causal, input.eos_token,
  output.out,
  output.attention_weights,
  output.logits
);
```

---

### ğŸ”§ Performance Optimizations

- SIMD vectorization (Vec<T>) for matmul, qk, softmax
- Softmax via LUT (512-entry quantized exponential)
- per-tile loop unrolling and cache reuse
- Separate LUT logic from kernel to enable preloading

---

### ğŸ“‚ Used by

- `int8_decoder.cpp`
- `CUDADecoder` fallback (for CPU mode)
- `cli/` â†’ `generate_cli.py`, `chat_cli.py`, `generate_batch.py`
- `api/`, `web/` backend (auto CPU fallback via backend_router)

---

### ğŸ“˜ References

- [FlashAttention Paper](https://arxiv.org/abs/2205.14135)
- [oneDNN INT8 Optimization Guide](https://github.com/oneapi-src/oneDNN)
- Vec<T> adapted from AVX2-style microkernel patterns

---

### âœ… Status

| Component                | Status |
|-------------------------|--------|
| Float32 Attention        | âœ… Stable |
| INT8 Attention           | âœ… Stable |
| Rotary / Causal / Masking| âœ… Ready |
| Top-k / Top-p Filter     | âœ… Integrated |
| EOS support              | âœ… Supported |
| Vectorization (Vec<T>)   | âœ… Cross-platform |
| Decoder integration      | âœ… Done |
